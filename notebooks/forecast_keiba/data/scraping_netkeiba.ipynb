{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notebook only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kedro\n",
    "from kedro.framework.context import load_context\n",
    "proj_path = '../../../' \n",
    "context = load_context(proj_path)\n",
    "# df = catalog.load(\"XXX\")\n",
    "parameters = context.params\n",
    "\n",
    "# import from src\n",
    "sys.path.append(\"./../../../src/forecast_keiba/\")\n",
    "from data.scraping_netkeiba import get_race_url_by_year_and_mon\n",
    "from data.scraping_netkeiba import get_race_url\n",
    "from data.scraping_netkeiba import scrape_race_span\n",
    "from data.scraping_netkeiba import scrape_id\n",
    "from data.scraping_netkeiba import scrape_race_info\n",
    "from data.scraping_netkeiba import scrape_race_results\n",
    "from data.scraping_netkeiba import scrape_horse_results\n",
    "from data.scraping_netkeiba import HorseResults\n",
    "from data.scraping_netkeiba import scrape_peds\n",
    "from data.scraping_netkeiba import add_blood_data\n",
    "from data.scraping_netkeiba import scrape_jockey_results\n",
    "from data.scraping_netkeiba import get_race_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import文\n",
    "メイン関数実行のため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from os import path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select,WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_netkeiba(parameters):\n",
    "    print(\"start scraping_netkeiba\")\n",
    "    # race urlの全量を取得\n",
    "    get_race_url(parameters)\n",
    "    \n",
    "    url_dir = '../../../data/raw/race_url/'\n",
    "\n",
    "    if not os.path.exists(url_dir):\n",
    "        os.makedirs(url_dir)\n",
    "    \n",
    "    dt_now = datetime.datetime.now()\n",
    "    race_url_list = []\n",
    "    for year in range(parameters[\"scrape_year_start\"], parameters[\"scrape_year_end\"]+1):\n",
    "        if year != dt_now.year:\n",
    "            for month in range(1,13):\n",
    "                path = url_dir+str(year)+'-'+str(month)+'.txt'\n",
    "                with open(path) as f:\n",
    "                    race_url_list += f.readlines()\n",
    "        else:\n",
    "            for month in range(1,dt_now.month):\n",
    "                path = url_dir+str(year)+'-'+str(month)+'.txt'\n",
    "                with open(path) as f:\n",
    "                    race_url_list += f.readlines()\n",
    "    race_id_list = []\n",
    "    for url in race_url_list:\n",
    "        race_id_list.append(url[-14:-2])\n",
    "    race_id_list = race_id_list[:parameters[\"scraping_limit\"]]\n",
    "        \n",
    "    # 初回判定\n",
    "    if os.path.exists('../../../data/raw/race_results_df.pickle') != True:\n",
    "        flag = False\n",
    "        add_blood = get_race_data(race_id_list,flag)\n",
    "        print(\"FINISH!!!\")\n",
    "        return\n",
    "\n",
    "    race_results_df = pd.read_pickle('../../../data/raw/race_results_df.pickle')\n",
    "    got_race_id_list = set(list(race_results_df.index))\n",
    "    difference_id_list = set(race_id_list) ^ got_race_id_list\n",
    "    \n",
    "    #なぜか失敗する\n",
    "    if '201305030305' in difference_id_list:\n",
    "        difference_id_list.remove('201305030305')\n",
    "    if '201805010107' in difference_id_list:\n",
    "        difference_id_list.remove('201805010107')\n",
    "    if '201709050706' in difference_id_list:\n",
    "        difference_id_list.remove('201709050706')\n",
    "    if '201808030406' in difference_id_list:\n",
    "        difference_id_list.remove('201808030406')\n",
    "    if '201005050810' in difference_id_list:\n",
    "        difference_id_list.remove('201005050810')\n",
    "    if '201009040210' in difference_id_list:\n",
    "        difference_id_list.remove('201009040210')\n",
    "    if '201006040811' in difference_id_list:\n",
    "        difference_id_list.remove('201006040811')\n",
    "    if '201008060611' in difference_id_list:\n",
    "        difference_id_list.remove('201008060611')\n",
    "\n",
    "        \n",
    "    if len(difference_id_list) > 0:\n",
    "        flag = True\n",
    "        race_results_dif_df = get_race_data(difference_id_list,flag)\n",
    "        race_results_df = pd.concat([race_results_df, race_results_dif_df])\n",
    "        race_results_df.to_pickle('../../../data/raw/race_results_df.pickle')\n",
    "        return race_results_df\n",
    "    else:\n",
    "        flag = False\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start scraping_netkeiba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/forecast-keiba/src/forecast_keiba/pipelines/base/../../data/scraping_netkeiba.py:117: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome('chromedriver', chrome_options=options) # 去年までのデータ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_race_url_by_year_and_mon year: 2020 month: 9\n",
      "already have 120 urls (2020 9)\n",
      "レース結果取得中\n",
      "scrape_race_results race_id: 201905010201\n",
      "scrape_race_results race_id: 201905010202\n",
      "scrape_race_results race_id: 201905010203\n",
      "馬情報取得中\n",
      "ジョッキー情報取得中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py:618: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ結合中\n",
      "血統情報取得中\n",
      "FINISH!!!\n"
     ]
    }
   ],
   "source": [
    "def main(parameters):\n",
    "    return scraping_netkeiba(parameters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecastkeiba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
