{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レースID作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for year in range(2019,2020,1):\n",
    "    for place in range(1, 11, 1):\n",
    "        for kai in range(1, 6, 1):\n",
    "            for day in range(1, 9, 1):\n",
    "                for r in range(1, 13, 1):\n",
    "                    race_id = (\n",
    "                        str(year)\n",
    "                        + str(place).zfill(2)\n",
    "                        + str(kai).zfill(2)\n",
    "                        + str(day).zfill(2)\n",
    "                        + str(r).zfill(2)\n",
    "                    )\n",
    "                    race_id_list.append(race_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース結果取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            \n",
    "            if len(df) < 3:\n",
    "                continue\n",
    "            # horse_idとjockey_idをスクレイピング\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "            )\n",
    "\n",
    "            for a in horse_a_list:\n",
    "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                horse_id_list.append(horse_id[0])\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "            )\n",
    "            for a in jockey_a_list:\n",
    "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                jockey_id_list.append(jockey_id[0])\n",
    "\n",
    "            df[\"horse_id\"] = horse_id_list\n",
    "            df[\"jockey_id\"] = jockey_id_list\n",
    "            \n",
    "            # change コースid追加\n",
    "            df['course_id'] = [int(race_id[4:6])]*len(horse_id_list)\n",
    "            \n",
    "            race_results[race_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = scrape_race_results(race_id_list[:3])\n",
    "race_id_list = list(set(list(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in results:\n",
    "    results[key].index = [key] * len(results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.concat([results[key] for key in results], sort=False)\n",
    "results.to_pickle('results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_info(race_id_list):\n",
    "    race_infos = {}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            \n",
    "            df = pd.read_html(url)[0]\n",
    "            # change データ取得失敗対応\n",
    "            if len(df) < 3:\n",
    "                continue\n",
    "                \n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts = (\n",
    "                soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "            )\n",
    "            info = re.findall(r'\\w+', texts)\n",
    "            info_dict = {}\n",
    "            for text in info:\n",
    "                if text in [\"芝\", \"ダート\"]:\n",
    "                    info_dict[\"race_type\"] = text\n",
    "                if \"障\" in text:\n",
    "                    info_dict[\"race_type\"] = \"障害\"\n",
    "                if \"m\" in text:\n",
    "                    info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[0])\n",
    "                if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                    info_dict[\"ground_state\"] = text\n",
    "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                    info_dict[\"weather\"] = text\n",
    "                if \"年\" in text and \"月\" in text and \"日\" in text: \n",
    "                    info_dict[\"date\"] = text\n",
    "                # change コース特性追加\n",
    "                if \"右\" in text:\n",
    "                    info_dict[\"course_type\"] = \"right\"\n",
    "                if \"左\" in text:\n",
    "                    info_dict[\"course_type\"] = \"left\"\n",
    "                if \"直線\" in text:\n",
    "                    info_dict[\"course_type\"] = \"straight\"\n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "race_infos = scrape_race_info(race_id_list)\n",
    "df_race_infos = pd.DataFrame(race_infos.values(), index=race_infos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_addinfo = results.merge(df_race_infos,left_index=True,right_index=True,how='inner')\n",
    "results_addinfo['date'] = pd.to_datetime(results_addinfo['date'],format='%Y年%m月%d日')\n",
    "results_addinfo.to_pickle('results_addinfo.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_addinfo = pd.read_pickle('results_addinfo.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "horse_id_list = results_addinfo['horse_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list, pre_horse_id=[]):\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in pre_horse_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            ## add(生産地)\n",
    "            texts = soup.find(\"div\", attrs={\"class\": \"db_prof_area_02\"}).find_all(\"a\")\n",
    "            for text in texts:\n",
    "                if \"breeder\" in str(text):\n",
    "                    Borned_place = str(text)[str(text).find('e=\"')+3:str(text).find('\">')]\n",
    "            \n",
    "            df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "                \n",
    "            df[\"Borned_place\"] = Borned_place\n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "horse_results = scrape_horse_results(horse_id_list)\n",
    "for key in horse_results:\n",
    "    horse_results[key].index = [key] * len(horse_results[key])\n",
    "df = pd.concat([horse_results[key] for key in horse_results])\n",
    "df.to_pickle('horse_results.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬生産地追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borned_place_list = []\n",
    "for i in range(len(results_addinfo)):\n",
    "    borned_place_list.append(list(set(list(horse_results[results_addinfo['horse_id'][i]][\"Borned_place\"])))[0])\n",
    "\n",
    "results_addinfo[\"Borned_place\"] = borned_place_list\n",
    "results_addinfo.to_pickle('results_addinfo.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 未実装項目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('horse_results.pickle')\n",
    "results_addinfo = pd.read_pickle('results_addinfo.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エラー回避のために実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_addinfo = results_addinfo[~(results_addinfo['着順'].astype(str).str.contains('\\D'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詳細戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付','着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['着順'].fillna(0, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "\n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    # change 馬の最高賞金追加\n",
    "    def max_money(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        max_money = filtered_df.groupby(level=0)[['賞金']].max()\n",
    "        return max_money.rename(columns={'賞金':'最高賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, n_samples), left_on='horse_id',\\\n",
    "                             right_index=True, how='left').merge(self.max_money(horse_id_list, date, 'all'), left_on='horse_id',\\\n",
    "                             right_index=True, how='left')\n",
    "        return merged_df\n",
    "\n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hr = HorseResults(df)\n",
    "results_5R = hr.merge_all(results_addinfo, n_samples=5)\n",
    "results_5R.to_pickle('results_5R.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 血統データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_peds(horse_id_list, pre_peds={}):\n",
    "    peds = pre_peds\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in peds.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            generations = {}\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return peds\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    \n",
    "    #target_columns以外にカテゴリ変数があれば、ダミー変数にする\n",
    "    df2 = pd.get_dummies(df2)\n",
    "\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "\n",
    "    return df2\n",
    "\n",
    "def add_blood_data(horse_id_list,df):\n",
    "    peds = scrape_peds(horse_id_list)\n",
    "    peds = pd.concat([peds[horse_id] for horse_id in peds], axis=1).T\n",
    "    peds = peds.add_prefix('peds_')\n",
    "    categorical_columns = ['horse_id'] + list(peds.columns)\n",
    "    df = df.merge(peds,left_on='horse_id', right_index=True, how='left')\n",
    "    df = process_categorical(df, categorical_columns)\n",
    "    df.drop(['horse_id'],axis = 1,inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理項目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_last(results):\n",
    "    df = results.copy()\n",
    "    \n",
    "    df = df[~(df['着順'].astype(str).str.contains('\\D'))]\n",
    "    df['着順'] = df['着順'].astype(int)\n",
    "    df['性'] = df['性齢'].map(lambda x:str(x)[0])\n",
    "    \n",
    "    # chaneg 馬の所属追加\n",
    "    df['所属'] = df['調教師'].map(lambda x:str(x)[1])\n",
    "    df['年齢'] = df['性齢'].map(lambda x:str(x)[1:]).astype(int)\n",
    "    df['体重'] = df['馬体重'].str.split('(',expand = True)[0].astype(int)\n",
    "    df['体重変化'] = df['馬体重'].str.split('(',expand = True)[1].str[:-1]\n",
    "    \n",
    "    # change 体重変化をint型へ\n",
    "    object_to_int = [int(s) for s in list(df['体重変化'])]\n",
    "    df['体重変化'] = object_to_int\n",
    "    df['単勝'] = df['単勝'].astype(float)\n",
    "    \n",
    "    # change 計不があるレースは消去\n",
    "    drop_lines = list(df.query('馬体重 == \"計不\"').index)\n",
    "    df = df.drop(index=drop_lines)\n",
    "    \n",
    "    ## 未実装項目\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(df['Borned_place'])\n",
    "    df['Borned_place'] = le.transform(df['Borned_place'])\n",
    "    \n",
    "    df.drop(['タイム'],axis=1,inplace=True)\n",
    "    df.drop(['着差'],axis=1,inplace=True)\n",
    "    df.drop(['調教師'],axis=1,inplace=True)\n",
    "    df.drop(['性齢'],axis=1,inplace=True)\n",
    "    df.drop(['馬体重'],axis=1,inplace=True)\n",
    "    df.drop(['馬名'],axis=1,inplace=True)\n",
    "    df.drop(['騎手'],axis=1,inplace=True)\n",
    "    df.drop(['単勝'],axis=1,inplace=True)\n",
    "    df.drop(['人気'],axis=1,inplace=True)\n",
    "    #df.drop(['horse_id'],axis=1,inplace=True)\n",
    "    df.drop(['jockey_id'],axis=1,inplace=True)\n",
    "    df['rank'] = df['着順'].map(lambda x: x if x < 4 else 4)\n",
    "    return df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習とテストに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,test_size):\n",
    "    sorted_id_list = df.sort_values('date').index.unique()\n",
    "    train_id_list = sorted_id_list[:round(len(sorted_id_list)*(1-test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list)*(1-test_size)):]\n",
    "    train = df.loc[train_id_list]\n",
    "    test = df.loc[test_id_list]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_5R = preprocessing_last(results_5R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_5R.to_pickle('results_5R.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_5R = pd.read_pickle('results_5R.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_blood = add_blood_data(horse_id_list,results_5R)\n",
    "add_blood.to_pickle('add_blood.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blood = pd.read_pickle('add_blood.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = split_data(add_blood,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_1 = train['rank'].value_counts()[1]\n",
    "rank_2 = train['rank'].value_counts()[2]\n",
    "rank_3 = train['rank'].value_counts()[3]\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={1:rank_1,2:rank_2,3:rank_3,4:rank_1},random_state=71)\n",
    "\n",
    "X_train = train.drop(['着順','date','rank'],axis=1)\n",
    "y_train = train['rank']\n",
    "X_test = test.drop(['着順','date','rank'],axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "#X_train_rus,y_train_rus = rus.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配ブースティング決定木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 2,\n",
    "    \"n_estimators\": 80,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"random_state\": 100,\n",
    "}\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values,y_train.values)\n",
    "\n",
    "print(lgb_clf.score(X_train,y_train),lgb_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(\n",
    "    {\"features\": X_train.columns, \"importance\": lgb_clf.feature_importances_}\n",
    ")\n",
    "importances.sort_values(\"importance\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lgb_clf, open('lightgbm.pickle', 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open('lightgbm.pickle', 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回収率計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def scrape_return_tables(race_id_list, pre_return_tables={}):\n",
    "    return_tables = pre_return_tables\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in return_tables.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            f = urlopen(url)\n",
    "            html = f.read()\n",
    "            html = html.replace(b'<br />', b'br')\n",
    "            dfs = pd.read_html(html)\n",
    "            return_tables[race_id] = pd.concat([dfs[1], dfs[2]])\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except:\n",
    "            break\n",
    "    return return_tables\n",
    "\n",
    "return_tables = scrape_return_tables(race_id_list)\n",
    "for key in return_tables:\n",
    "    return_tables[key].index = [key] * len(return_tables[key])\n",
    "return_tables = pd.concat([return_tables[key] for key in return_tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Return:\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "\n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        wins = fukusho[1].str.split('br', expand=True)#.drop([3], axis=1)\n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "\n",
    "        returns = fukusho[2].str.split('br', expand=True)#.drop([3], axis=1)\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "\n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "        return df.fillna(0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, return_tables):\n",
    "        self.model = model\n",
    "        self.fukusho = Return(return_tables).fukusho\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p<threshold else 1 for p in y_pred]\n",
    "\n",
    "    def score(self, y_true, X):\n",
    "        return roc_auc_score(y_true, self.predict_proba(X))\n",
    "\n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({\"features\": X.columns, \n",
    "                                    \"importance\": self.model.feature_importances_})\n",
    "        return importances.sort_values(\"importance\", ascending=False)[:n_display]\n",
    "\n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['馬番']]\n",
    "        pred_table['pred'] = self.predict(X, threshold)\n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred']==1]['馬番']\n",
    "        else:\n",
    "            return pred_table\n",
    "\n",
    "    def calculate_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        money = -100 * len(pred_table)\n",
    "        df = self.fukusho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        for i in range(3):\n",
    "            money += df[df['win_{}'.format(i)]==df['馬番']]['return_{}'.format(i)].sum()\n",
    "        return money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me = ModelEvaluator(lgb_clf, return_tables)\n",
    "\n",
    "gain = {}\n",
    "n_samples = 100\n",
    "for i in tqdm(range(n_samples)):\n",
    "    threshold = i / n_samples\n",
    "    gain[threshold] = me.calculate_return(X_test, threshold)\n",
    "pd.Series(gain).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## グラフの見方\n",
    "## 左に行けば行くほど全部の馬にかけるということ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
