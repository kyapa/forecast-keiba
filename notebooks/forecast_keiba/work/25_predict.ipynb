{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from selenium.webdriver import Chrome, ChromeOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース詳細情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_info(soup):\n",
    "    text_race_data = str(soup.find('div',attrs={'class':'RaceData01'}))\n",
    "    race_data = soup.find('div',attrs={'class':'RaceData01'})\n",
    "        \n",
    "    whether_text = [text_race_data[text_race_data.find(\"天候\")+3:text_race_data.find('<span class=\"Icon_Weather')]]\n",
    "    course_type_text = [text_race_data[text_race_data.find(\"(\")+1:text_race_data.find(\")\")]]\n",
    "    ground_type_text = [race_data.find_all('span')[0].text]\n",
    "    ground_state_text = [race_data.find_all('span')[2].text[race_data.find_all('span')[2].text.find(\":\")+1:]]\n",
    "\n",
    "    race_info = ground_state_text+ ground_type_text + whether_text + course_type_text + day\n",
    "    \n",
    "    info_dict = {}\n",
    "\n",
    "    title = soup.find('title').text\n",
    "    if \"G1\" in title:\n",
    "        info_dict[\"Grade\"] = \"1\"\n",
    "    elif \"G2\" in title:\n",
    "        info_dict[\"Grade\"] = \"2\"\n",
    "    elif \"G3\" in title:\n",
    "        info_dict[\"Grade\"] = \"3\"\n",
    "    else:\n",
    "        info_dict[\"Grade\"] = \"4\"\n",
    "\n",
    "    for text in race_info:\n",
    "        if \"芝\" in text:\n",
    "            info_dict[\"race_type\"] = '芝'\n",
    "        if \"ダ\" in text:\n",
    "            info_dict[\"race_type\"] = 'ダート'\n",
    "        if \"障\" in text:\n",
    "            info_dict[\"race_type\"] = \"障害\"\n",
    "        if \"m\" in text:\n",
    "            info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[0])\n",
    "        if text in [\"良\",\"稍重\", \"重\", \"不良\"]:\n",
    "            info_dict[\"ground_state\"] = text\n",
    "        if text in \"稍\":\n",
    "            info_dict[\"ground_state\"] = \"稍重\"\n",
    "        if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "            info_dict[\"weather\"] = text\n",
    "        if \"年\" in text:\n",
    "            info_dict[\"date\"] = text\n",
    "        if \"右\" in text:\n",
    "            info_dict[\"course_type\"] = \"right\"\n",
    "        if \"左\" in text:\n",
    "            info_dict[\"course_type\"] = \"left\"\n",
    "        if \"直線\" in text:\n",
    "            info_dict[\"course_type\"] = \"straight\"\n",
    "\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬とジョッキーのID取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_id(soup,id_name):\n",
    "    id_list = []\n",
    "\n",
    "    if id_name == \"horse\":\n",
    "        words  = soup.find_all(\"td\", attrs={\"class\": \"HorseInfo\"})\n",
    "        for word in words:\n",
    "            id_list.append(word.find(\"a\").get('href')[-10:])\n",
    "    elif id_name == \"jockey\":\n",
    "        words  = soup.find_all(\"td\", attrs={\"class\": \"Jockey\"})\n",
    "        for word in words:\n",
    "            id_list.append(word.find(\"a\").get('href')[-6:-1])\n",
    "        \n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出馬表作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_horse_table(df):\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp[\"枠番\"] = df[(\"枠\",\"枠\")].values\n",
    "    df_tmp[\"馬番\"] = df[(\"馬番\",\"馬番\")].values\n",
    "    df_tmp[\"馬名\"] = df[(\"馬名\",\"馬名\")].values\n",
    "    df_tmp[\"性齢\"] = df[(\"性齢\",\"性齢\")].values\n",
    "    df_tmp[\"斤量\"] = df[(\"斤量\",\"斤量\")].values\n",
    "    df_tmp[\"騎手\"] = df[(\"騎手\",\"騎手\")].values\n",
    "    df_tmp[\"厩舎\"] = df[(\"厩舎\",\"厩舎\")].values\n",
    "    df_tmp[\"馬体重(増減)\"] = df[(\"馬体重(増減)\",\"馬体重(増減)\")].values\n",
    "\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出走間隔取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_span(race_id,df_length):\n",
    "    url = \"https://race.netkeiba.com/race/shutuba_past.html?race_id=\"+race_id+\"&rf=shutuba_submenu\"\n",
    "    df = pd.read_html(url)[0]\n",
    "    texts = list(df[\"馬名\"].values)\n",
    "    race_span = []\n",
    "    for text in texts:\n",
    "        words = text.split( )\n",
    "        flag = False\n",
    "        for word in words:\n",
    "            if \"中\" in word and \"週\" in word:\n",
    "                race_span.append(int(word[1:-1]))\n",
    "                flag = True\n",
    "                continue\n",
    "            elif \"連闘\" in word:\n",
    "                race_span.append(1)\n",
    "                flag = True\n",
    "                continue\n",
    "        if flag == False:\n",
    "            race_span.append(0)\n",
    "    if len(df) != df_length:\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    time.sleep(0.1)\n",
    "    return race_span,flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  レース情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_predict(race_id_list, pre_race_tables={}):\n",
    "    race_tables = pre_race_tables\n",
    "    race_infos = {}\n",
    "    for race_id in race_id_list:\n",
    "        url = \"https://race.netkeiba.com/race/shutuba.html?race_id=\" + race_id + \"&rf=race_submenu\"\n",
    "        df = pd.read_html(url)[0]\n",
    "        table = make_horse_table(df)\n",
    "\n",
    "        html = requests.get(url)\n",
    "        html.encoding = \"EUC-JP\"\n",
    "        soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "        race_span,flag = scrape_race_span(race_id,len(df))\n",
    "        if flag != True:\n",
    "            print(\"length error\",race_id)\n",
    "            continue\n",
    "        table[\"race_span\"] = race_span\n",
    "\n",
    "        horse_id_list = scrape_id(soup,\"horse\")\n",
    "        jockey_id_list = scrape_id(soup,\"jockey\")\n",
    "\n",
    "        if len(horse_id_list) != len(df) or len(jockey_id_list) != len(df):\n",
    "            continue\n",
    "\n",
    "        table[\"horse_id\"] = horse_id_list\n",
    "        table[\"jockey_id\"] = jockey_id_list\n",
    "        table['course_id'] = [int(race_id[4:6])]*len(df)\n",
    "\n",
    "        info_dict = {}\n",
    "        info_dict = scrape_race_info(soup)\n",
    "        race_tables[race_id] = table\n",
    "        race_infos[race_id] = info_dict\n",
    "\n",
    "    return race_tables,race_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list, pre_horse_id=[]):\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in pre_horse_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            \n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            texts = soup.find(\"div\", attrs={\"class\": \"db_prof_area_02\"}).find_all(\"a\")\n",
    "            for text in texts:\n",
    "                if \"breeder\" in str(text):\n",
    "                    Borned_place = str(text)[str(text).find('e=\"')+3:str(text).find('\">')]\n",
    "            \n",
    "            df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "\n",
    "            df[\"Borned_place\"] = Borned_place\n",
    "\n",
    "            horse_results[horse_id] = df\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の詳細戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付','着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['着順'].fillna(0, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        #df.drop(['日付'], axis=1, inplace=True)\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "\n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    # change 馬の最高賞金追加\n",
    "    def max_money(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        max_money = filtered_df.groupby(level=0)[['賞金']].max()\n",
    "        return max_money.rename(columns={'賞金':'最高賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, 3), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.average(horse_id_list, date, 5), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.average(horse_id_list, date, \"all\"), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.max_money(horse_id_list, date, 'all'), left_on='horse_id',right_index=True, how='left')\n",
    "        return merged_df\n",
    "\n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 血統データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_peds(horse_id_list, pre_peds={}):\n",
    "    peds = pre_peds\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in peds.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            generations = {}\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return peds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリー化およびダミー化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    \n",
    "    #target_columns以外にカテゴリ変数があれば、ダミー変数にする\n",
    "    df2 = pd.get_dummies(df2)\n",
    "\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 血統データ結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blood_data(horse_id_list,df):\n",
    "    peds = scrape_peds(horse_id_list)\n",
    "    peds = pd.concat([peds[horse_id] for horse_id in peds], axis=1).T\n",
    "    peds = peds.add_prefix('peds_')\n",
    "    df = df.merge(peds,left_on='horse_id', right_index=True, how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ジョッキー情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jockey_results(jockey_id_list, pre_jockey_id=[]):\n",
    "    jockey_results = {}\n",
    "    for jockey_id in tqdm(jockey_id_list):\n",
    "        if jockey_id in pre_jockey_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/jockey/result/' + jockey_id + '/'\n",
    "            df = pd.read_html(url)[0][['勝率','連対率','複勝率']][:1]\n",
    "            jockey_results[jockey_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return jockey_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_predict(df):\n",
    "    df['性'] = df['性齢'].map(lambda x:str(x)[0])\n",
    "    df['年齢'] = df['性齢'].map(lambda x:str(x)[1:]).astype(int)\n",
    "    df['体重'] = df['馬体重(増減)'].str.split('(',expand = True)[0].astype(int)\n",
    "    df['体重変化'] = df['馬体重(増減)'].str.split('(',expand = True)[1].str[:-1]\n",
    "    df.loc[df['体重変化'] == \"前計不\", '体重変化'] = 0\n",
    "    object_to_int = [int(s) for s in list(df['体重変化'])]\n",
    "    df['体重変化'] = object_to_int\n",
    "    df['枠番'] = df['枠番'].astype(int)\n",
    "    df['馬番'] = df['馬番'].astype(int)\n",
    "    df['斤量'] = df['斤量'].astype(float)\n",
    "    df['斤量'] = df['斤量'].astype(int)\n",
    "    df['所属'] = df['厩舎'].map(lambda x:str(x)[:2])\n",
    "    df = df.replace('栗東', '西')\n",
    "    df = df.replace('美浦', '東')\n",
    "    df = df.replace('地方', '地')\n",
    "    df = df.replace('海外', '海')\n",
    "    \n",
    "    horse_name = df[\"馬名\"]\n",
    "    \n",
    "    df.drop(['性齢','馬体重(増減)'],axis = 1,inplace = True)\n",
    "    df.drop(['馬名'],axis=1,inplace=True)\n",
    "    df.drop(['騎手'],axis=1,inplace=True)\n",
    "    df.drop(['厩舎'],axis=1,inplace=True)\n",
    "    df.drop(['horse_id'],axis=1,inplace=True)\n",
    "    df.drop(['jockey_id'],axis=1,inplace=True)\n",
    "    df.drop(['date'],axis=1,inplace=True)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(df['Borned_place'])\n",
    "    df['Borned_place'] = le.transform(df['Borned_place'])\n",
    "\n",
    "    return df.fillna(0),horse_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理とダミー化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_race_predict(df):\n",
    "    preprocess_df,horse_name = preprocessing_predict(df)\n",
    "    target_columns = []\n",
    "    for i in range(62):\n",
    "        target_columns.append('peds_'+str(i))\n",
    "    preprocess_df = process_categorical(preprocess_df, target_columns)\n",
    "    return preprocess_df,horse_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と推論でデータに差がないか比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_df(df):\n",
    "    df_predict = df.copy()\n",
    "\n",
    "    df_train = pd.read_pickle('pickle_data/train_data.pickle')\n",
    "    df_train.drop(['date'],axis=1,inplace=True)\n",
    "    df_train.drop(['着順'],axis=1,inplace=True)\n",
    "    df_train.drop(['rank'],axis=1,inplace=True)\n",
    "    horse_count = len(df_predict)\n",
    "    train_colums_list = list(df_train.columns.values)\n",
    "    predict_colums_list = list(df_predict.columns.values)\n",
    "    for t_column in train_colums_list:\n",
    "        if t_column not in predict_colums_list:\n",
    "            df_predict[t_column] = [0]*horse_count\n",
    "            \n",
    "    return df_predict.fillna(0).reindex(columns=train_colums_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(race_id_list):\n",
    "\n",
    "    print(\"レース結果取得中\")\n",
    "    race_tables,race_infos = scrape_race_predict(race_id_list)\n",
    "    for key in race_tables:\n",
    "        race_tables[key].index = [key] * len(race_tables[key])\n",
    "    race_tables = pd.concat([race_tables[key] for key in race_tables], sort=False)\n",
    "    df_infos = pd.DataFrame(race_infos.values(), index=race_infos.keys())\n",
    "    predict_addinfo = race_tables.merge(df_infos,left_index=True,right_index=True,how='inner')\n",
    "    predict_addinfo['date'] = pd.to_datetime(predict_addinfo['date'],format='%Y年%m月%d日')\n",
    "\n",
    "    print(\"馬情報取得中\")\n",
    "    horse_id_list = predict_addinfo['horse_id'].unique()\n",
    "    horse_results = scrape_horse_results(horse_id_list)\n",
    "    for key in horse_results:\n",
    "        horse_results[key].index = [key] * len(horse_results[key])\n",
    "    df_horse_results = pd.concat([horse_results[key] for key in horse_results])\n",
    "    \n",
    "    print(\"ジョッキー情報取得中\")\n",
    "    jockey_id_list = predict_addinfo['jockey_id'].unique()\n",
    "    jockey_results = scrape_jockey_results(jockey_id_list)\n",
    "    for key in jockey_results:\n",
    "        jockey_results[key].index = [key] * len(jockey_results[key])\n",
    "    df_jockey_results = pd.concat([jockey_results[key] for key in jockey_results])\n",
    "    predict_addinfo = predict_addinfo.merge(df_jockey_results,left_on='jockey_id',right_index=True,how='left')\n",
    "\n",
    "    print(\"馬の生産地取得\")\n",
    "    borned_place_list = []\n",
    "    for i in range(len(predict_addinfo)):\n",
    "        borned_place_list.append(list(set(list(horse_results[predict_addinfo['horse_id'][i]][\"Borned_place\"])))[0])\n",
    "    predict_addinfo[\"Borned_place\"] = borned_place_list\n",
    "\n",
    "    print(\"データ結合中\")\n",
    "    hr = HorseResults(df_horse_results)\n",
    "    predict_all = hr.merge_all(predict_addinfo, n_samples=5)\n",
    "        \n",
    "    print(\"血統情報取得中\")\n",
    "    add_blood_predict = add_blood_data(horse_id_list,predict_all)\n",
    "    preprocess_df,horse_name = preprocess_race_predict(add_blood_predict)\n",
    "    predict_data = compare_df(preprocess_df)\n",
    "    \n",
    "    return predict_data,horse_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 札幌:1,函館:2,福島:3,新潟:4,東京:5,中山:6,中京:7,京都:8,阪神:9,小倉:10\n",
    "\n",
    "year = 2020\n",
    "place = 9\n",
    "kai = 2\n",
    "day = 4\n",
    "race_id_table = []\n",
    "for r in range(1, 13, 1):\n",
    "    race_id = (str(year)+str(place).zfill(2)+str(kai).zfill(2)+str(day).zfill(2)+str(r).zfill(2))\n",
    "    race_id_table.append(race_id)\n",
    "\n",
    "#race_id_list = [race_id_table[10]]\n",
    "race_id_list = [\"202004030811\"]\n",
    "day = ['2020年9月6日']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_data, horse_name = predict(race_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('pickle_data/lightgbm.pickle', 'rb'))\n",
    "result_proba = loaded_model.predict_proba(predict_data)\n",
    "result = loaded_model.predict(predict_data)\n",
    "category1 = []\n",
    "category2 = []\n",
    "category3 = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    #print('馬番',i+1,\"予想カテゴリー\",result[i],result_proba[i])\n",
    "    category1.append(result_proba[i][0])\n",
    "    category2.append(result_proba[i][1])\n",
    "    category3.append(result_proba[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"馬名\",\"予想カテゴリー\",\"カテゴリー1\",\"カテゴリー2\",\"カテゴリー3\"]\n",
    "idx = [i for i in range(1,len(result)+1)]\n",
    "df = pd.DataFrame(index=idx, columns=cols)\n",
    "df[\"馬名\"] = list(horse_name)\n",
    "df[\"予想カテゴリー\"] = result\n",
    "df[\"カテゴリー1\"] = category1\n",
    "df[\"カテゴリー2\"] = category2\n",
    "df[\"カテゴリー3\"] = category3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('カテゴリー1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"1\", \"2\", \"3\"]\n",
    "for i in range(len(result_proba)):\n",
    "    plt.figure(figsize=(6, 4), dpi=72,\n",
    "                 facecolor='skyblue', linewidth=10, edgecolor='green')\n",
    "    plt.title(\"馬番\"+str(i+1)+\": \"+list(horse_name)[i], fontname=\"MS Gothic\")\n",
    "    plt.pie(np.array(result_proba[i]), labels=label, counterclock=False, startangle=90, autopct=\"%1.1f%%\")\n",
    "    plt.figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
