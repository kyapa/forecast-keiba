{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出走間隔取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_span(race_id,df_length):\n",
    "    url = \"https://race.netkeiba.com/race/shutuba_past.html?race_id=\"+race_id+\"&rf=shutuba_submenu\"\n",
    "    df = pd.read_html(url)[0]\n",
    "    texts = list(df[\"馬名\"].values)\n",
    "    race_span = []\n",
    "    for text in texts:\n",
    "        words = text.split( )\n",
    "        flag = False\n",
    "        for word in words:\n",
    "            if \"中\" in word and \"週\" in word:\n",
    "                race_span.append(int(word[1:-1]))\n",
    "                flag = True\n",
    "                continue\n",
    "            elif \"連闘\" in word:\n",
    "                race_span.append(1)\n",
    "                flag = True\n",
    "                continue\n",
    "        if flag == False:\n",
    "            race_span.append(0)\n",
    "    if len(df) != df_length:\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    time.sleep(0.1)\n",
    "    return race_span,flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬とジョッキーのID取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_id(soup,id_name):\n",
    "    id_list = []\n",
    "    words = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "        \"a\", attrs={\"href\": re.compile(\"^/\"+id_name)}\n",
    "    )\n",
    "\n",
    "    for word in words:\n",
    "        id = re.findall(r\"\\d+\", word[\"href\"])\n",
    "        id_list.append(id[0])\n",
    "        \n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース詳細情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_info(soup):\n",
    "    texts = (soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text + \n",
    "            soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text)\n",
    "    title = soup.find('title').text\n",
    "    info = re.findall(r'\\w+', texts)\n",
    "    info_dict = {}\n",
    "\n",
    "    if \"G1\" in title:\n",
    "        info_dict[\"Grade\"] = \"1\"\n",
    "    elif \"G2\" in title:\n",
    "        info_dict[\"Grade\"] = \"2\"\n",
    "    elif \"G3\" in title:\n",
    "        info_dict[\"Grade\"] = \"3\"\n",
    "    else:\n",
    "        info_dict[\"Grade\"] = \"4\"\n",
    "\n",
    "    for text in info:\n",
    "        if text in [\"芝\", \"ダート\"]:\n",
    "            info_dict[\"race_type\"] = text\n",
    "        if \"障\" in text:\n",
    "            info_dict[\"race_type\"] = \"障害\"\n",
    "        if \"m\" in text:\n",
    "            info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[0])\n",
    "        if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "            info_dict[\"ground_state\"] = text\n",
    "        if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "            info_dict[\"weather\"] = text\n",
    "        if \"年\" in text and \"月\" in text and \"日\" in text: \n",
    "            info_dict[\"date\"] = text\n",
    "        # change コース特性追加\n",
    "        if \"右\" in text:\n",
    "            info_dict[\"course_type\"] = \"right\"\n",
    "        if \"左\" in text:\n",
    "            info_dict[\"course_type\"] = \"left\"\n",
    "        if \"直線\" in text:\n",
    "            info_dict[\"course_type\"] = \"straight\"\n",
    "            \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レース結果を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    race_infos = {}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            if len(df) < 3:\n",
    "                continue\n",
    "                \n",
    "            race_span,flag = scrape_race_span(race_id,len(df))\n",
    "            if flag != True:\n",
    "                print(\"length error\",race_id)\n",
    "                continue\n",
    "            df[\"race_span\"] = race_span\n",
    "            \n",
    "            horse_id_list = scrape_id(soup,\"horse\")\n",
    "            jockey_id_list = scrape_id(soup,\"jockey\")\n",
    "\n",
    "            if len(horse_id_list) != len(df) or len(jockey_id_list) != len(df):\n",
    "                continue\n",
    "                        \n",
    "            df[\"horse_id\"] = horse_id_list\n",
    "            df[\"jockey_id\"] = jockey_id_list\n",
    "            \n",
    "            # change コースid追加\n",
    "            df['course_id'] = [int(race_id[4:6])]*len(df)\n",
    "            \n",
    "            race_results[race_id] = df\n",
    "\n",
    "            info_dict = {}\n",
    "            info_dict = scrape_race_info(soup)\n",
    "            \n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_results,race_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list, pre_horse_id=[]):\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in pre_horse_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            ## add(生産地)\n",
    "            texts = soup.find(\"div\", attrs={\"class\": \"db_prof_area_02\"}).find_all(\"a\")\n",
    "            for text in texts:\n",
    "                if \"breeder\" in str(text):\n",
    "                    Borned_place = str(text)[str(text).find('e=\"')+3:str(text).find('\">')]\n",
    "            \n",
    "            df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "                \n",
    "            df[\"Borned_place\"] = Borned_place\n",
    "            \n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の詳細戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付','着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['着順'].fillna(0, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "\n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    # change 馬の最高賞金追加\n",
    "    def max_money(self, horse_id_list, date, n_samples='all'):\n",
    "        self.horse_results.reindex(horse_id_list, axis=1)\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        max_money = filtered_df.groupby(level=0)[['賞金']].max()\n",
    "        return max_money.rename(columns={'賞金':'最高賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, 3), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.average(horse_id_list, date, 5), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.average(horse_id_list, date, \"all\"), left_on='horse_id',right_index=True, how='left')\\\n",
    "                      .merge(self.max_money(horse_id_list, date, 'all'), left_on='horse_id',right_index=True, how='left')\n",
    "        return merged_df\n",
    "\n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の血統取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_peds(horse_id_list, pre_peds={}):\n",
    "    peds = pre_peds\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in peds.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            generations = {}\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return peds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 血統データ結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blood_data(horse_id_list,df):\n",
    "    peds = scrape_peds(horse_id_list)\n",
    "    peds = pd.concat([peds[horse_id] for horse_id in peds], axis=1).T\n",
    "    peds = peds.add_prefix('peds_')\n",
    "    df = df.merge(peds,left_on='horse_id', right_index=True, how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ジョッキー情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jockey_results(jockey_id_list, pre_jockey_id=[]):\n",
    "    jockey_results = {}\n",
    "    for jockey_id in tqdm(jockey_id_list):\n",
    "        if jockey_id in pre_jockey_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/jockey/result/' + jockey_id + '/'\n",
    "            df = pd.read_html(url)[0][['勝率','連対率','複勝率']][:1]\n",
    "            jockey_results[jockey_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return jockey_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ取得手順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_data(race_id_list,flag):\n",
    "    print(\"レース結果取得中\")\n",
    "    results,race_infos = scrape_race_results(race_id_list)\n",
    "    for key in results:\n",
    "        results[key].index = [key] * len(results[key])\n",
    "    results = pd.concat([results[key] for key in results], sort=False)\n",
    "    df_race_infos = pd.DataFrame(race_infos.values(), index=race_infos.keys())\n",
    "    results_addinfo = results.merge(df_race_infos,left_index=True,right_index=True,how='inner')\n",
    "    results_addinfo['date'] = pd.to_datetime(results_addinfo['date'],format='%Y年%m月%d日')\n",
    "    \n",
    "    print(\"馬情報取得中\")\n",
    "    horse_id_list = results_addinfo['horse_id'].unique()\n",
    "    horse_results = scrape_horse_results(horse_id_list)\n",
    "    for key in horse_results:\n",
    "        horse_results[key].index = [key] * len(horse_results[key])\n",
    "    df_horse_results = pd.concat([horse_results[key] for key in horse_results])\n",
    "    \n",
    "    print(\"ジョッキー情報取得中\")\n",
    "    jockey_id_list = results_addinfo['jockey_id'].unique()\n",
    "    jockey_results = scrape_jockey_results(jockey_id_list)\n",
    "    for key in jockey_results:\n",
    "        jockey_results[key].index = [key] * len(jockey_results[key])\n",
    "    df_jockey_results = pd.concat([jockey_results[key] for key in jockey_results])\n",
    "    results_addinfo = results_addinfo.merge(df_jockey_results,left_on='jockey_id',right_index=True,how='left')\n",
    "\n",
    "    borned_place_list = []\n",
    "    for i in range(len(results_addinfo)):\n",
    "        borned_place_list.append(list(set(list(horse_results[results_addinfo['horse_id'][i]][\"Borned_place\"])))[0])\n",
    "    results_addinfo[\"Borned_place\"] = borned_place_list\n",
    "    \n",
    "    results_addinfo = results_addinfo[~(results_addinfo['着順'].astype(str).str.contains('\\D'))]\n",
    "    drop_lines = list(results_addinfo.query('馬体重 == \"計不\"').index)\n",
    "    results_addinfo = results_addinfo.drop(index=drop_lines)\n",
    "\n",
    "    print(\"データ結合中\")\n",
    "    hr = HorseResults(df_horse_results)\n",
    "    results_5R = hr.merge_all(results_addinfo, n_samples=5)\n",
    "    \n",
    "    print(\"血統情報取得中\")\n",
    "    add_blood = add_blood_data(horse_id_list,results_5R)\n",
    "    if flag == True:\n",
    "        add_blood.to_pickle('pickle_data/race_results_dif_df.pickle')\n",
    "        return add_blood\n",
    "    else:\n",
    "        add_blood.to_pickle('pickle_data/race_results_df.pickle')\n",
    "        return add_blood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_netkeiba():\n",
    "    dt_now = datetime.datetime.now()\n",
    "    race_url_list = []\n",
    "    for year in range(2010,2020):\n",
    "        \n",
    "        if year != dt_now.year:\n",
    "            for month in range(1,13):\n",
    "                path = 'race_url/'+str(year)+'-'+str(month)+'.txt'\n",
    "                with open(path) as f:\n",
    "                    race_url_list += f.readlines()\n",
    "        else:\n",
    "            for month in range(1,dt_now.month):\n",
    "                path = 'race_url/'+str(year)+'-'+str(month)+'.txt'\n",
    "                with open(path) as f:\n",
    "                    race_url_list += f.readlines()\n",
    "    race_id_list = []\n",
    "    for url in race_url_list:\n",
    "        race_id_list.append(url[-14:-2])\n",
    "        \n",
    "    # 初回判定\n",
    "    if os.path.exists('pickle_data/race_results_df.pickle') != True:\n",
    "        flag = False\n",
    "        add_blood = get_race_data(race_id_list,flag)\n",
    "        print(\"FINISH!!!\")\n",
    "        return\n",
    "\n",
    "    race_results_df = pd.read_pickle('pickle_data/race_results_df.pickle')\n",
    "    got_race_id_list = set(list(race_results_df.index))\n",
    "    difference_id_list = set(race_id_list) ^ got_race_id_list\n",
    "    \n",
    "    #なぜか失敗する\n",
    "    if '201305030305' in difference_id_list:\n",
    "        difference_id_list.remove('201305030305')\n",
    "    if '201805010107' in difference_id_list:\n",
    "        difference_id_list.remove('201805010107')\n",
    "    if '201709050706' in difference_id_list:\n",
    "        difference_id_list.remove('201709050706')\n",
    "    if '201808030406' in difference_id_list:\n",
    "        difference_id_list.remove('201808030406')\n",
    "    if '201005050810' in difference_id_list:\n",
    "        difference_id_list.remove('201005050810')\n",
    "    if '201009040210' in difference_id_list:\n",
    "        difference_id_list.remove('201009040210')\n",
    "    if '201006040811' in difference_id_list:\n",
    "        difference_id_list.remove('201006040811')\n",
    "    if '201008060611' in difference_id_list:\n",
    "        difference_id_list.remove('201008060611')\n",
    "        \n",
    "    if len(difference_id_list) > 0:\n",
    "        flag = True\n",
    "        race_results_dif_df = get_race_data(difference_id_list,flag)\n",
    "        race_results_df = pd.concat([race_results_df, race_results_dif_df])\n",
    "        race_results_df.to_pickle('pickle_data/race_results_df.pickle')\n",
    "        print(\"FINISH!!!\")\n",
    "        return\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"FINISH!!!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraping_netkeiba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
