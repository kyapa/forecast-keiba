{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium.webdriver import Chrome, ChromeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = ['202004020211']\n",
    "day = [\"2020年7月26日\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出馬表取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShutubaTable:\n",
    "    def __init__(self):\n",
    "        self.shutuba_table = pd.DataFrame()\n",
    "    \n",
    "    def scrape_shutuba_table(self, race_id_list):\n",
    "        options = ChromeOptions()\n",
    "        #driver = Chrome(options=options)\n",
    "        driver = Chrome()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url  = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "            driver.get(url)\n",
    "            elements = driver.find_elements_by_class_name('HorseList')\n",
    "            for element in elements:\n",
    "                row = []\n",
    "                tds = element.find_elements_by_tag_name('td')\n",
    "                for td in tds:\n",
    "                    row.append(td.text)\n",
    "                self.shutuba_table = self.shutuba_table.append(pd.Series(row, name=race_id))\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ShutubaTable()\n",
    "st.scrape_shutuba_table(race_id_list)\n",
    "race_plans_df = st.shutuba_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出馬表整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_race_plans_df(race_plans_df):\n",
    "    df = race_plans_df.copy()  \n",
    "    df = df.rename(columns={0 :'枠番',1:'馬番',3:'馬名',4:'性齢',5:'斤量',6:'騎手',7:'厩舎',8:'馬体重',9:'単勝',10:'人気'})\n",
    "    df['性'] = df['性齢'].map(lambda x:str(x)[0])\n",
    "    df['年齢'] = df['性齢'].map(lambda x:str(x)[1:]).astype(int)\n",
    "    df['体重'] = df['馬体重'].str.split('(',expand = True)[0].astype(int)\n",
    "    df['体重変化'] = df['馬体重'].str.split('(',expand = True)[1].str[:-1]\n",
    "    df.loc[df['体重変化'] == \"前計不\", '体重変化'] = 0\n",
    "    object_to_int = [int(s) for s in list(df['体重変化'])]\n",
    "    df['体重変化'] = object_to_int\n",
    "    df['枠番'] = df['枠番'].astype(int)\n",
    "    df['馬番'] = df['馬番'].astype(int)\n",
    "    df['斤量'] = df['斤量'].astype(float)\n",
    "    df['斤量'] = df['斤量'].astype(int)\n",
    "    df['所属'] = df['厩舎'].map(lambda x:str(x)[:2])\n",
    "    df['単勝'] = df['単勝'].astype(float)\n",
    "    df['人気'] = df['人気'].astype(float)\n",
    "    horse_count = len(df)\n",
    "    df['course_id'] = [int(race_id_list[0][4:6])]*horse_count\n",
    "    df.drop([2,11,12,'性齢','馬体重'],axis = 1,inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_table = preprocessing_race_plans_df(race_plans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対象レース情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_info(race_id_list):\n",
    "    for race_id in race_id_list:\n",
    "        url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "        html = requests.get(url)\n",
    "        html.encoding = \"EUC-JP\"\n",
    "        soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "        text_race_data = str(soup.find('div',attrs={'class':'RaceData01'}))\n",
    "        race_data = soup.find('div',attrs={'class':'RaceData01'})\n",
    "        \n",
    "        whether_text = [text_race_data[text_race_data.find(\"天候\")+3:text_race_data.find('<span class=\"Icon_Weather')]]\n",
    "        course_type_text = [text_race_data[text_race_data.find(\"(\")+1:text_race_data.find(\")\")]]\n",
    "        ground_type_text = [race_data.find_all('span')[0].text]\n",
    "        ground_state_text = [race_data.find_all('span')[2].text[race_data.find_all('span')[2].text.find(\":\")+1:]]\n",
    "\n",
    "        race_info = ground_state_text+ ground_type_text + whether_text + course_type_text + day\n",
    "        \n",
    "        info_dict = {}\n",
    "        race_infos = {}\n",
    "        for text in race_info:\n",
    "            if \"芝\" in text:\n",
    "                info_dict[\"race_type\"] = '芝'\n",
    "            if \"ダ\" in text:\n",
    "                info_dict[\"race_type\"] = 'ダート'\n",
    "            if \"障\" in text:\n",
    "                info_dict[\"race_type\"] = \"障害\"\n",
    "            if \"m\" in text:\n",
    "                info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[0])\n",
    "            if text in [\"良\", \"稍\",\"稍重\", \"重\", \"不良\"]:\n",
    "                info_dict[\"ground_state\"] = text\n",
    "            if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                info_dict[\"weather\"] = text\n",
    "            if \"年\" in text:\n",
    "                info_dict[\"date\"] = text\n",
    "            if \"右\" in text:\n",
    "                info_dict[\"course_type\"] = \"right\"\n",
    "            if \"左\" in text:\n",
    "                info_dict[\"course_type\"] = \"left\"\n",
    "            if \"直線\" in text:\n",
    "                info_dict[\"course_type\"] = \"straight\"\n",
    "\n",
    "        race_infos[race_id] = info_dict\n",
    "        return race_infos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_infos = get_race_info(race_id_list)\n",
    "df_infos = pd.DataFrame(race_infos.values(), index=race_infos.keys())\n",
    "predict_add_race_info = horse_table.merge(df_infos,left_index=True,right_index=True,how='inner')     \n",
    "predict_add_race_info['date'] = pd.to_datetime(predict_add_race_info['date'],format='%Y年%m月%d日')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬idとジョッキーid追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_horse_jockey_id(predict_add_race_info,race_id_list):\n",
    "    df = predict_add_race_info.copy() \n",
    "    for race_id in race_id_list:\n",
    "        url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "        html = requests.get(url)\n",
    "        html.encoding = \"EUC-JP\"\n",
    "        soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    \n",
    "        horse_id_list = []\n",
    "        horse_soup_list  = soup.find_all(\"td\", attrs={\"class\": \"HorseInfo\"})\n",
    "\n",
    "        for horse_soup in horse_soup_list:\n",
    "            horse_id_list.append(horse_soup.find(\"a\").get('href')[-10:])\n",
    "    \n",
    "        jockey_id_list = []\n",
    "        jockey_soup_list  = soup.find_all(\"td\", attrs={\"class\": \"Jockey\"})\n",
    "\n",
    "        for jockey_soup in jockey_soup_list:\n",
    "            jockey_id_list.append(jockey_soup.find(\"a\").get('href')[-6:-1])\n",
    "        \n",
    "        df['horse_id'] = horse_id_list\n",
    "        df['jockey_id'] = jockey_id_list\n",
    "    \n",
    "    return df,horse_id_list,jockey_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_addinfo,horse_id_list,jockey_id_list = add_horse_jockey_id(predict_add_race_info,race_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 馬の過去戦績取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list, pre_horse_id=[]):\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in pre_horse_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_results = scrape_horse_results(horse_id_list)\n",
    "for key in horse_results:\n",
    "    horse_results[key].index = [key] * len(horse_results[key])\n",
    "df_horse_results = pd.concat([horse_results[key] for key in horse_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "\n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    # change 馬の最高賞金追加\n",
    "    def max_money(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        max_money = filtered_df.groupby(level=0)[['賞金']].max()\n",
    "        return max_money.rename(columns={'賞金':'最高賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, n_samples), left_on='horse_id',\n",
    "                             right_index=True, how='left').merge(self.max_money(horse_id_list, date, n_samples), left_on='horse_id',\n",
    "                             right_index=True, how='left')\n",
    "        return merged_df\n",
    "\n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HorseResults(df_horse_results)\n",
    "predict_all = hr.merge_all(predict_addinfo, n_samples='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予想データ整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_predict(predict_all):\n",
    "    df = predict_all.copy()\n",
    "    df.drop(['馬名'],axis=1,inplace=True)\n",
    "    df.drop(['騎手'],axis=1,inplace=True)\n",
    "    df.drop(['厩舎'],axis=1,inplace=True)\n",
    "    df.drop(['horse_id'],axis=1,inplace=True)\n",
    "    df.drop(['jockey_id'],axis=1,inplace=True)\n",
    "    df.drop(['date'],axis=1,inplace=True)\n",
    "    df = df.replace('栗東', '西')\n",
    "    df = df.replace('美浦', '東')\n",
    "    df = df.replace('地方', '地')\n",
    "    df = df.replace('海外', '海')\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_predict = preprocessing_predict(predict_all)\n",
    "horse_count = len(preprocessing_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データ整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.read_pickle('results_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_results(results):\n",
    "    df = results.copy()\n",
    "    df.drop(['着順'],axis=1,inplace=True)\n",
    "    df.drop(['馬名'],axis=1,inplace=True)\n",
    "    df.drop(['騎手'],axis=1,inplace=True)\n",
    "    df.drop(['horse_id'],axis=1,inplace=True)\n",
    "    df.drop(['jockey_id'],axis=1,inplace=True)\n",
    "    df.drop(['date'],axis=1,inplace=True)\n",
    "    return df.fillna(0)\n",
    "\n",
    "preprocessing_results = preprocessing_results(results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([preprocessing_results, preprocessing_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.get_dummies(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 予想対象レース抽出\n",
    "pred_data_new =  pred_data[len(pred_data)-horse_count:len(pred_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムフォレストで予想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## たまにエラーする\n",
    "#pred_data_new = pred_data_new.drop(['ground_state_稍'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('11_race_id_to_horse_info.pickle', 'rb'))\n",
    "result = loaded_model.predict(pred_data_new)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
