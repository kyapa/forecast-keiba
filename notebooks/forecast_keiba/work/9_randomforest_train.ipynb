{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for year in range(2019,2020,1):\n",
    "    for place in range(1, 11, 1):\n",
    "        for kai in range(1, 6, 1):\n",
    "            for day in range(1, 9, 1):\n",
    "                for r in range(1, 13, 1):\n",
    "                    race_id = (\n",
    "                        str(year)\n",
    "                        + str(place).zfill(2)\n",
    "                        + str(kai).zfill(2)\n",
    "                        + str(day).zfill(2)\n",
    "                        + str(r).zfill(2)\n",
    "                    )\n",
    "                    race_id_list.append(race_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            \n",
    "            # change データ取得失敗対応\n",
    "            if len(df) < 3:\n",
    "                continue\n",
    "            # horse_idとjockey_idをスクレイピング\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "            )\n",
    "\n",
    "            for a in horse_a_list:\n",
    "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                horse_id_list.append(horse_id[0])\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "            )\n",
    "            for a in jockey_a_list:\n",
    "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                jockey_id_list.append(jockey_id[0])\n",
    "\n",
    "            df[\"horse_id\"] = horse_id_list\n",
    "            df[\"jockey_id\"] = jockey_id_list\n",
    "            \n",
    "            # change コースid追加\n",
    "            df['course_id'] = [int(race_id[4:6])]*len(horse_id_list)\n",
    "            \n",
    "            race_results[race_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = scrape_race_results(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in results:\n",
    "    results[key].index = [key] * len(results[key])\n",
    "results = pd.concat([results[key] for key in results], sort=False)\n",
    "results.to_pickle('results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_info(race_id_list):\n",
    "    race_infos = {}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            \n",
    "            df = pd.read_html(url)[0]\n",
    "            # change データ取得失敗対応\n",
    "            if len(df) < 3:\n",
    "                continue\n",
    "                \n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts = (\n",
    "                soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "            )\n",
    "            info = re.findall(r'\\w+', texts)\n",
    "            info_dict = {}\n",
    "            for text in info:\n",
    "                if text in [\"芝\", \"ダート\"]:\n",
    "                    info_dict[\"race_type\"] = text\n",
    "                if \"障\" in text:\n",
    "                    info_dict[\"race_type\"] = \"障害\"\n",
    "                if \"m\" in text:\n",
    "                    info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[0])\n",
    "                if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                    info_dict[\"ground_state\"] = text\n",
    "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                    info_dict[\"weather\"] = text\n",
    "                if \"年\" in text:\n",
    "                    info_dict[\"date\"] = text\n",
    "                # change コース特性追加\n",
    "                if \"右\" in text:\n",
    "                    info_dict[\"course_type\"] = \"right\"\n",
    "                if \"左\" in text:\n",
    "                    info_dict[\"course_type\"] = \"left\"\n",
    "                if \"直線\" in text:\n",
    "                    info_dict[\"course_type\"] = \"straight\"\n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_infos = scrape_race_info(race_id_list)\n",
    "df_race_infos = pd.DataFrame(race_infos.values(), index=race_infos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_addinfo = results.merge(df_race_infos,left_index=True,right_index=True,how='inner')\n",
    "results_addinfo.to_pickle('results_addinfo.pickle')\n",
    "results_addinfo = pd.read_pickle('results_addinfo.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 計不があるレースは消去\n",
    "drop_lines = list(results_addinfo.query('馬体重 == \"計不\"').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_addinfo_new = results_addinfo.drop(index=drop_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_rf(results):\n",
    "    df = results.copy()\n",
    "    \n",
    "    df = df[~(df['着順'].astype(str).str.contains('\\D'))]\n",
    "    df['着順'] = df['着順'].astype(int)\n",
    "    df['性'] = df['性齢'].map(lambda x:str(x)[0])\n",
    "    \n",
    "    # chaneg 馬の所属追加\n",
    "    df['所属'] = df['調教師'].map(lambda x:str(x)[1])\n",
    "    df['年齢'] = df['性齢'].map(lambda x:str(x)[1:]).astype(int)\n",
    "    df['体重'] = df['馬体重'].str.split('(',expand = True)[0].astype(int)\n",
    "    df['体重変化'] = df['馬体重'].str.split('(',expand = True)[1].str[:-1]\n",
    "    \n",
    "    # change 体重変化をint型へ\n",
    "    object_to_int = [int(s) for s in list(df['体重変化'])]\n",
    "    df['体重変化'] = object_to_int\n",
    "    \n",
    "    \n",
    "    df['単勝'] = df['単勝'].astype(float)\n",
    "    df['date'] = pd.to_datetime(df['date'],format='%Y年%m月%d日')\n",
    "    \n",
    "    df.drop(['タイム','着差','調教師','性齢','馬体重'],axis = 1,inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocessing_rf(results_addinfo_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_id_list = test['horse_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list, pre_horse_id=[]):\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in pre_horse_id:\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_results = scrape_horse_results(horse_id_list)\n",
    "for key in horse_results:\n",
    "    horse_results[key].index = [key] * len(horse_results[key])\n",
    "df_horse_results = pd.concat([horse_results[key] for key in horse_results])\n",
    "df_horse_results.to_pickle('horse_results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "\n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "\n",
    "        self.horse_results = df\n",
    "\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "\n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "\n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    # change 馬の最高賞金追加\n",
    "    def max_money(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        max_money = filtered_df.groupby(level=0)[['賞金']].max()\n",
    "        return max_money.rename(columns={'賞金':'最高賞金_{}R'.format(n_samples)})\n",
    "\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, n_samples), left_on='horse_id',\n",
    "                             right_index=True, how='left').merge(self.max_money(horse_id_list, date, n_samples), left_on='horse_id',\n",
    "                             right_index=True, how='left')\n",
    "        return merged_df\n",
    "\n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HorseResults(df_horse_results)\n",
    "results_all = hr.merge_all(test, n_samples='all')\n",
    "results_all.to_pickle('results_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_last(results):\n",
    "    df = results.copy()\n",
    "    df.drop(['馬名'],axis=1,inplace=True)\n",
    "    df.drop(['騎手'],axis=1,inplace=True)\n",
    "    df.drop(['horse_id'],axis=1,inplace=True)\n",
    "    df.drop(['jockey_id'],axis=1,inplace=True)\n",
    "    df['rank'] = df['着順'].map(lambda x: x if x<4 else 4)\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,test_size):\n",
    "    sorted_id_list = df.sort_values('date').index.unique()\n",
    "    train_id_list = sorted_id_list[:round(len(sorted_id_list)*(1-test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list)*(1-test_size)):]\n",
    "    train = df.loc[train_id_list]\n",
    "    test = df.loc[test_id_list]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.get_dummies(preprocessing_last(results_all))\n",
    "sample.to_pickle('sample.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_pickle('sample.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = split_data(sample,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_1 = train['rank'].value_counts()[1]\n",
    "rank_2 = train['rank'].value_counts()[2]\n",
    "rank_3 = train['rank'].value_counts()[3]\n",
    "\n",
    "model = LogisticRegression()\n",
    "rus = RandomUnderSampler(sampling_strategy={1:rank_1,2:rank_2,3:rank_3,4:rank_1},random_state=71)\n",
    "\n",
    "X_train = train.drop(['着順','date','rank'],axis=1)\n",
    "y_train = train['rank']\n",
    "X_test = test.drop(['着順','date','rank'],axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "X_train_rus,y_train_rus = rus.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train_rus,y_train_rus)\n",
    "\n",
    "print(clf.score(X_train,y_train),clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('11_race_id_to_horse_info.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('11_race_id_to_horse_info.pickle', 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
